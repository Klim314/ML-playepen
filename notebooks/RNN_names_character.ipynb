{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence, pack_sequence\n",
    "\n",
    "import random\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "data_path = \"../data/names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;\"\n",
    "\n",
    "def unicode2ascii(s):\n",
    "    # Converts a unicode string to an ascii string\n",
    "    return \"\".join(char for char in unicodedata.normalize(\"NFD\", s)\n",
    "                   if unicodedata.category(char) != \"Mn\"\n",
    "                   and char in all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20074 16059 4015\n",
      "[{'name': 'janimov', 'language': 'russian'}, {'name': 'adrol', 'language': 'english'}, {'name': 'jahno', 'language': 'russian'}, {'name': 'binnington', 'language': 'english'}, {'name': 'tikhin', 'language': 'russian'}] [{'name': 'andryuhin', 'language': 'russian'}, {'name': 'fadeechev', 'language': 'russian'}, {'name': 'ukhabin', 'language': 'russian'}, {'name': 'danisevich', 'language': 'russian'}, {'name': 'tallett', 'language': 'english'}]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Random seed\n",
    "random.seed(1235)\n",
    "# Load raw data directly\n",
    "n_languages = len(os.listdir(data_path))\n",
    "names = []\n",
    "categories = set()\n",
    "for i in os.listdir(data_path):\n",
    "    language, ext = os.path.splitext(i)\n",
    "    language = language.lower()\n",
    "    with open(os.path.join(data_path, i), encoding=\"utf-8\") as f:\n",
    "        for i in f:\n",
    "            names.append({\"name\": i.lower().strip(),\n",
    "                          \"language\": language})\n",
    "    categories.add(language)\n",
    "# make categories indexable\n",
    "categories = list(categories)\n",
    "\n",
    "# Clean all names by converting to ascii\n",
    "for namedict in names:\n",
    "    namedict[\"name\"] = unicode2ascii(namedict[\"name\"])\n",
    "    \n",
    "# split the data into train and test\n",
    "random.shuffle(names)\n",
    "# 80/20 train test split\n",
    "total = len(names)\n",
    "trainsplit = 0.8\n",
    "point = int(total * trainsplit)\n",
    "train_data, test_data = names[:point], names[point:]\n",
    "print(total, len(train_data), len(test_data))\n",
    "print(train_data[:5], test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]) torch.Size([3, 56])\n",
      "abc\n",
      "PackedSequence(data=tensor([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), batch_sizes=tensor([ 2,  2,  2,  1]))\n",
      "torch.Size([2, 4, 56])\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning functions\n",
    "def letter2index(l):\n",
    "    return all_letters.find(l)\n",
    "\n",
    "def index2letter(i):\n",
    "    return all_letters[i]\n",
    "\n",
    "# Turn an \n",
    "def line2tensor(line):\n",
    "    \"\"\"\n",
    "    @args\n",
    "        line (str): Line to be converted to tensor form\n",
    "    @returns:\n",
    "        line_length x n_letters tensor containing one hot encodings\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(line), len(all_letters))\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][letter2index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def tensor2line(tensor):\n",
    "    # Takes in a L, 56 tensor and returns the string that it represents\n",
    "    holder = []\n",
    "    for onehot in tensor:\n",
    "        holder.append(index2letter(torch.max(onehot, dim=0)[1]))\n",
    "    return \"\".join(holder)\n",
    "\n",
    "def tensors2line(tensors):\n",
    "    holder = []\n",
    "    for tensor in tensors:\n",
    "        holder.append(tensor2line(tensor))\n",
    "    return holder\n",
    "        \n",
    "        \n",
    "\n",
    "def lines2tensor(lines):\n",
    "    # Processes a string iterable and converts it to a padded, packed tensor ready for RNN ingestion\n",
    "    indatas = [line2tensor(i) for i in lines]\n",
    "    indatas.sort(key=lambda x: x.shape[0], reverse=True)\n",
    "#     print([i.shape for i in indatas])\n",
    "    indatas = pack_sequence(indatas)\n",
    "#     seq_lengths = [i.shape[1] for i in indatas]\n",
    "#     indatas = [i.view(-1, len(all_letters)) for i in indatas]\n",
    "#     indatas = pad_sequence(indatas, batch_first=True)\n",
    "#     indatas = pack_padded_sequence(indatas, seq_lengths, batch_first=True)\n",
    "    return indatas\n",
    "\n",
    "t = line2tensor(\"abc\")\n",
    "print(t, t.shape)\n",
    "print(tensor2line(t))\n",
    "test_packed = lines2tensor([\"abc\", \"dhef\"])\n",
    "print(test_packed)\n",
    "print(pad_packed_sequence(test_packed, batch_first=True)[0].shape)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': ['bencivenni', 'bakaleinikov', 'antar', 'ghannam', 'makusev', 'shainurov', 'avtaev', 'pakhrin', 'yep', 'oneil', 'masih', 'quigley', 'agudov', 'achthoven', 'laraway', 'ahearn', 'peleev', 'tompkin', 'talbaev', 'giordano', 'boutros', 'teunissen', 'castro', 'ladyzhensky', 'adriyanoff', 'robert', 'levish', 'belin', 'oulton', 'deulin', 'mojaikin', 'guttridge'], 'language': ['italian', 'russian', 'arabic', 'arabic', 'russian', 'russian', 'russian', 'russian', 'chinese', 'english', 'arabic', 'english', 'russian', 'dutch', 'english', 'irish', 'russian', 'english', 'russian', 'italian', 'arabic', 'dutch', 'spanish', 'russian', 'russian', 'english', 'russian', 'russian', 'english', 'russian', 'russian', 'english']}\n"
     ]
    }
   ],
   "source": [
    "# Create datasets that produce tensor data\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.data[x]\n",
    "    \n",
    "train_dataset = NameDataset(train_data)\n",
    "test_dataset = NameDataset(test_data)\n",
    "train_dataset = DataLoader(batch_size=32, shuffle=True, dataset=train_dataset)\n",
    "test_dataset = DataLoader(batch_size=32, shuffle=True, dataset=test_dataset)\n",
    "print(next(train_dataset.__iter__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'janimov', 'language': 'russian'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural network\n",
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn = nn.RNN(batch_first=True, hidden_size=hidden_size, input_size=input_size)\n",
    "        # Output layer to convert to the expected values\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, packed):\n",
    "        _, hx = self.rnn(packed)\n",
    "        # Output is of form 1, seq_count, cats\n",
    "        output = self.output(hx)\n",
    "        # Shape it to seq_count, cats\n",
    "        return output.view(-1, self.output_size)\n",
    "        \n",
    "test_net = BasicRNN(len(all_letters), 128, len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([ 1,  1,  1,  2,  2,  3]), batch_sizes=tensor([ 3,  2,  1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  2,  3],\n",
       "         [ 1,  2,  0],\n",
       "         [ 1,  0,  0]]), tensor([ 3,  2,  1]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure out how sequence padding and packing work in pytorch\n",
    "# Create a test array of three sequences of length (1, 3, 2)\n",
    "testdata = [torch.tensor(i) for i in [[1],\n",
    "                                      [1, 2, 3],\n",
    "                                      [1, 2]]]\n",
    "# Sort the sequences in decreasing order cause pytorch wants it that way\n",
    "testdata.sort(reverse=True, key=lambda x: len(x))\n",
    "\n",
    "# seq_lengths = [len(i) for i in testdata]\n",
    "\n",
    "# This whole part outmoded by\n",
    "# # Pad the data with zeros so that these variable length sequences are of the same length\n",
    "# # batch_first = True cause I like the batches to be first\n",
    "# padded = pad_sequence(testdata, batch_first=True)\n",
    "# print(padded)\n",
    "# # Pack the sequence\n",
    "# packed = pack_padded_sequence(padded, seq_lengths, batch_first=True)\n",
    "# print(packed)\n",
    "packed = pack_sequence(testdata)\n",
    "print(packed)\n",
    "pad_packed_sequence(packed, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(train_dataset.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salamanca', 'windsor', 'mozhin']\n",
      "PackedSequence(data=tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  ...,  0.,  0.,  0.]]), batch_sizes=tensor([ 3,  3,  3,  3,  3,  3,  2,  1,  1]))\n",
      "RNN output HX:  torch.Size([1, 3, 128])\n",
      "torch.Size([3, 18])\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 5.1837,  5.0056,  6.1426,  5.3347,  5.1278,  6.3156,  5.2563,\n",
      "          5.9159,  5.6195,  5.7671,  5.5178,  5.4083,  5.2057,  6.0908,\n",
      "          6.0268,  5.3849,  5.6299,  5.0669],\n",
      "        [ 4.7370,  4.9136,  6.5397,  5.2144,  5.2133,  6.0095,  5.0403,\n",
      "          6.1807,  5.5723,  5.7727,  5.5756,  5.1264,  5.1509,  5.6095,\n",
      "          6.2172,  6.3172,  5.5561,  5.2537],\n",
      "        [ 5.2140,  4.6636,  6.1361,  5.3398,  5.4716,  6.0890,  5.1352,\n",
      "          6.0093,  5.7362,  5.6305,  5.2479,  5.0521,  5.3651,  5.7918,\n",
      "          6.7263,  5.9539,  5.3692,  5.0683]])\n",
      "torch.Size([3, 18])\n",
      "These should all be one\n",
      "tensor([ 1.0000,  1.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Test out the pytorch RNN implementation\n",
    "# RNN params\n",
    "hidden_size = 128\n",
    "input_size = len(all_letters)\n",
    "output_size = n_languages\n",
    "\n",
    "# testdata\n",
    "names = sample_data[\"name\"][:3]\n",
    "print(names)\n",
    "\n",
    "# Convert to tensor sequences\n",
    "\n",
    "# Pack the names to handle variable length input\n",
    "packed_names = lines2tensor(names)\n",
    "\n",
    "\n",
    "# Example of how the RNN class works\n",
    "net = nn.RNN(batch_first=True, hidden_size=hidden_size, input_size=input_size)\n",
    "\n",
    "print(packed_names)\n",
    "\n",
    "packed_output, hx = net(packed_names)\n",
    "print(\"RNN output HX: \", hx.shape)\n",
    "# Feed into a linear layer to get output (untrained)\n",
    "nn.Linear(128, 10)(hx)\n",
    "\n",
    "# Basic RNN based on the RNN class\n",
    "net = BasicRNN(hidden_size=hidden_size, input_size=input_size, output_size = output_size)\n",
    "output = net(packed_names)\n",
    "\n",
    "print(output.shape)\n",
    "softmaxed = F.softmax(output, dim=1)\n",
    "print(softmaxed)\n",
    "print(softmaxed.shape)\n",
    "print(\"These should all be one\")\n",
    "print(softmaxed.sum(dim=1))\n",
    "\n",
    "# OLD\n",
    "# Test parallel RNN execution\n",
    "# pytorch RNN takes input of form (B, S, F) since batch_first is used\n",
    "\n",
    "\n",
    "# indatas = [line2tensor(i) for i in names]\n",
    "# indatas.sort(key=lambda x: x.shape[1], reverse=True)\n",
    "# seq_lengths = [i.shape[1] for i in indatas]\n",
    "# print(seq_lengths)\n",
    "# indatas = [i.view(-1, len(all_letters)) for i in indatas]\n",
    "# print([i.shape for i in indatas])\n",
    "# indatas = pad_sequence(indatas, batch_first=True)\n",
    "# print(indatas.shape)\n",
    "# indatas = pack_padded_sequence(indatas, seq_lengths, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0814,  0.1730, -0.0733,  0.0006,  0.0747, -0.0025, -0.0081,\n",
      "         -0.0308, -0.1275, -0.0787,  0.1349, -0.0764,  0.0239,  0.0004,\n",
      "         -0.0684, -0.0827, -0.0473,  0.1088],\n",
      "        [ 0.0813,  0.0670, -0.0555,  0.0175,  0.0432,  0.0138,  0.0881,\n",
      "          0.0149, -0.1118, -0.0031,  0.0067, -0.0811,  0.0815, -0.0060,\n",
      "         -0.0539, -0.0442, -0.0165,  0.0439],\n",
      "        [ 0.0738,  0.1236, -0.0318,  0.0284,  0.0561, -0.0045,  0.0712,\n",
      "         -0.0040, -0.0632, -0.0240,  0.0862, -0.0131,  0.0468, -0.0171,\n",
      "          0.0049, -0.0996, -0.0875,  0.0564]]) torch.Size([3, 18])\n",
      "tensor([ 1,  6,  1])\n",
      "['chinese', 'german', 'chinese']\n",
      "12\n",
      "tensor([ 12])\n",
      "tensor(0.9398)\n",
      "tensor(0.9233)\n"
     ]
    }
   ],
   "source": [
    "def tensor2index_tensor(tensor):\n",
    "    # Converts a tensor of outputs (one-hot encoded) to a tensor of indicies\n",
    "    # Here, we're expecting them to be of form (n, catsize), where n is the number of samples\n",
    "    # returns a tensor of form (n) containing the indices\n",
    "    return F.softmax(tensor, dim=1).max(dim=1)[1]\n",
    "\n",
    "def index_tensor2cat(index_tensor):\n",
    "    # Converts an index tensor to a list of categories\n",
    "    return [categories[i] for i in index_tensor]\n",
    "\n",
    "\n",
    "def cat2index(cat):\n",
    "    return categories.index(cat)\n",
    "\n",
    "def cat2index_tensor(cat):\n",
    "    dat = [cat2index(cat)]\n",
    "    return torch.tensor(dat, dtype=torch.long)\n",
    "\n",
    "def cats2index_tensor(cats):\n",
    "    dat = [cat2index(cat) for cat in cats]\n",
    "    return torch.tensor(dat, dtype=torch.long)\n",
    "\n",
    "# def index_tensor2tensor(index_tensor, cat_size=len(categories)):\n",
    "#     # Converts an index tensor to a one hot encoding of \n",
    "    \n",
    "\n",
    "print(output, output.shape)\n",
    "idx_tensor = tensor2indextensor(output)\n",
    "print(idx_tensor)\n",
    "print(index_tensor2cat(idx_tensor))\n",
    "print(cat2index(\"portuguese\"))\n",
    "print(cat2index_tensor(\"portuguese\"))\n",
    "# Note: Cross Entropy log does the log_softmax then a NLLoss. \n",
    "# Expects input of raw output values, target of just the expected index\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, cats2index_tensor([\"portuguese\", \"german\", \"chinese\"]))\n",
    "print(loss/3)\n",
    "loss = criterion(output, cats2index_tensor([\"chinese\", \"german\", \"chinese\"]))\n",
    "print(loss/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Batch: 99; Loss: 2.841360569000244\n",
      "Epoch: 0; Batch: 199; Loss: 2.8402509689331055\n",
      "Epoch: 0; Batch: 299; Loss: 2.8420932292938232\n",
      "Epoch: 0; Batch: 399; Loss: 2.839658498764038\n",
      "Epoch: 0; Batch: 499; Loss: 2.841223955154419\n",
      "Epoch: 1; Batch: 99; Loss: 2.7059319019317627\n",
      "Epoch: 1; Batch: 199; Loss: 2.646411418914795\n",
      "Epoch: 1; Batch: 299; Loss: 2.644838571548462\n",
      "Epoch: 1; Batch: 399; Loss: 2.6461188793182373\n",
      "Epoch: 1; Batch: 499; Loss: 2.644071340560913\n",
      "Epoch: 2; Batch: 99; Loss: 2.342114210128784\n",
      "Epoch: 2; Batch: 199; Loss: 2.2967958450317383\n",
      "Epoch: 2; Batch: 299; Loss: 2.302952766418457\n",
      "Epoch: 2; Batch: 399; Loss: 2.2789788246154785\n",
      "Epoch: 2; Batch: 499; Loss: 2.2747859954833984\n",
      "Epoch: 3; Batch: 99; Loss: 2.0026121139526367\n",
      "Epoch: 3; Batch: 199; Loss: 1.981723666191101\n",
      "Epoch: 3; Batch: 299; Loss: 1.9475895166397095\n",
      "Epoch: 3; Batch: 399; Loss: 1.956693410873413\n",
      "Epoch: 3; Batch: 499; Loss: 2.0010578632354736\n",
      "Epoch: 4; Batch: 99; Loss: 2.104062795639038\n",
      "Epoch: 4; Batch: 199; Loss: 2.034681797027588\n",
      "Epoch: 4; Batch: 299; Loss: 2.0074334144592285\n",
      "Epoch: 4; Batch: 399; Loss: 2.024960517883301\n",
      "Epoch: 4; Batch: 499; Loss: 2.0254852771759033\n",
      "Epoch: 5; Batch: 99; Loss: 2.008270502090454\n",
      "Epoch: 5; Batch: 199; Loss: 2.0061237812042236\n",
      "Epoch: 5; Batch: 299; Loss: 2.0464091300964355\n",
      "Epoch: 5; Batch: 399; Loss: 2.0245590209960938\n",
      "Epoch: 5; Batch: 499; Loss: 1.9892617464065552\n",
      "Epoch: 6; Batch: 99; Loss: 2.0308361053466797\n",
      "Epoch: 6; Batch: 199; Loss: 2.0050230026245117\n",
      "Epoch: 6; Batch: 299; Loss: 1.9949820041656494\n",
      "Epoch: 6; Batch: 399; Loss: 1.9754444360733032\n",
      "Epoch: 6; Batch: 499; Loss: 1.9968740940093994\n",
      "Epoch: 7; Batch: 99; Loss: 2.011903762817383\n",
      "Epoch: 7; Batch: 199; Loss: 1.928452730178833\n",
      "Epoch: 7; Batch: 299; Loss: 1.949670433998108\n",
      "Epoch: 7; Batch: 399; Loss: 1.9641815423965454\n",
      "Epoch: 7; Batch: 499; Loss: 1.9399091005325317\n",
      "Epoch: 8; Batch: 99; Loss: 2.005305051803589\n",
      "Epoch: 8; Batch: 199; Loss: 1.901363492012024\n",
      "Epoch: 8; Batch: 299; Loss: 1.937381625175476\n",
      "Epoch: 8; Batch: 399; Loss: 1.9841892719268799\n",
      "Epoch: 8; Batch: 499; Loss: 1.9109022617340088\n",
      "Epoch: 9; Batch: 99; Loss: 1.9544434547424316\n",
      "Epoch: 9; Batch: 199; Loss: 1.9242383241653442\n",
      "Epoch: 9; Batch: 299; Loss: 1.942891240119934\n",
      "Epoch: 9; Batch: 399; Loss: 1.9410814046859741\n",
      "Epoch: 9; Batch: 499; Loss: 1.927978515625\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "input_size = len(all_letters)\n",
    "output_size = n_languages\n",
    "net = BasicRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.005)\n",
    "# net.cuda()\n",
    "epochs = 10\n",
    "losses  = []\n",
    "running_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for batchnum, minibatch in enumerate(train_dataset):\n",
    "        # Zero out the previous gradients\n",
    "        optim.zero_grad()\n",
    "        names, languages = minibatch[\"name\"], minibatch[\"language\"]\n",
    "        # Create the data tensors to be used\n",
    "        input_tensor = lines2tensor(names)\n",
    "        target_tensor = cats2index_tensor(languages)\n",
    "        output = net(input_tensor)\n",
    "        loss = criterion(output, target_tensor)\n",
    "        # Minibatch optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss\n",
    "        # Print every 100 batches\n",
    "        if batchnum % 100 == 99:\n",
    "            losses.append(running_loss)\n",
    "            print(f\"Epoch: {epoch}; Batch: {batchnum}; Loss: {running_loss/100}\")\n",
    "            running_loss = 0\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1881/4015, 0\n",
      "Wrong: 2134/4015, 0\n"
     ]
    }
   ],
   "source": [
    "# test out our little network\n",
    "correct, wrong, total = 0, 0, 0\n",
    "for minibatch in test_dataset:\n",
    "    names, languages = minibatch[\"name\"], minibatch[\"language\"]\n",
    "    indata_tensor, target_tensor = lines2tensor(names), cats2index_tensor(languages)\n",
    "    # Raw outputs\n",
    "    output = net(indata_tensor)\n",
    "    output = tensor2index_tensor(output)\n",
    "    total += len(names)\n",
    "    correct += (output == target_tensor).sum()\n",
    "    wrong += (output != target_tensor).sum()\n",
    "\n",
    "print(f\"Correct: {correct}/{total}, {correct/total}\")\n",
    "print(f\"Wrong: {wrong}/{total}, {wrong/total}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x224c5d57588>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
